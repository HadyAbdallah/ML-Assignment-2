{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\n# Suppress SettingWithCopyWarning\nwarnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# Load the \"diabetes.csv\" dataset.\ndata = pd.read_csv('diabetes.csv')\n\n# The features and targets are separated\nx = data.drop(columns=['Outcome'])\ny = data[['Outcome']]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "# The data is shuffled and split into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=100)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "# Features are Normalized using Min-Max Scaling.\nx_train_max = x_train.max()\nx_train_min = x_train.min()\nrange_x_train = x_train_max - x_train_min\nx_test_scaled = (x_test - x_train_min) / range_x_train\nx_train_scaled = (x_train - x_train_min) / range_x_train",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "# Convert data to Numpy array\nx_train_np = x_train_scaled.to_numpy().reshape((-1, 8))\nx_test_np = x_test_scaled.to_numpy().reshape((-1, 8))\ny_train_np = y_train.to_numpy().reshape((-1, 1))\ny_test_np = y_test.to_numpy().reshape((-1, 1))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "# Function to calculate Euclidean Distance\ndef euclidean_distance(point1, point2):\n    return np.sqrt(np.sum((point1 - point2) ** 2))\n\n# Function for Distance-Weighted Voting\ndef distance_weighted_vote(distances):\n    weights = 1 / (distances + 1e-10)  # Adding a small constant to avoid division by zero\n    return weights / np.sum(weights)\n\n# Function to predict the class using KNN\ndef knn_predict(train_data, train_labels, test_instance, k):\n    distances = np.array([euclidean_distance(test_instance, train_instance) for train_instance in train_data])\n    sorted_indices = np.argsort(distances)\n\n    # Break ties using Distance-Weighted Voting\n    vote_weights = distance_weighted_vote(distances[sorted_indices[:k]])\n    class_votes = np.zeros(np.max(train_labels) + 1)\n\n    for i in range(k):\n        class_votes[train_labels[sorted_indices[i]]] += vote_weights[i]\n\n    predicted_class = np.argmax(class_votes)\n    return predicted_class\n\n# Function to evaluate KNN for a given k value\ndef knn_evaluate(train_data, train_labels, test_data, test_labels, k):\n    correct_count = 0\n\n    for i in range(len(test_data)):\n        predicted_class = knn_predict(train_data, train_labels, test_data[i], k)\n        if predicted_class == test_labels[i]:\n            correct_count += 1\n\n    accuracy = correct_count / len(test_data) * 100\n    return correct_count, len(test_data), accuracy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "# Set the range of k values for iterations\nk_values = [2, 3, 4 ,7 ,23]\naccuracies=[]\n# Perform iterations and print results\nfor k in k_values:\n    correct, total, accuracy = knn_evaluate(x_train_np, y_train_np, x_test_np, y_test_np, k)\n    print(f\"k value: {k}\")\n    print(f\"Number of correctly classified instances: {correct}\")\n    print(f\"Total number of instances: {total}\")\n    print(f\"Accuracy: {accuracy:.2f}%\\n\")\n    accuracies.append(accuracy)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "k value: 2\nNumber of correctly classified instances: 163\nTotal number of instances: 231\nAccuracy: 70.56%\n\nk value: 3\nNumber of correctly classified instances: 167\nTotal number of instances: 231\nAccuracy: 72.29%\n\nk value: 4\nNumber of correctly classified instances: 164\nTotal number of instances: 231\nAccuracy: 71.00%\n\nk value: 7\nNumber of correctly classified instances: 170\nTotal number of instances: 231\nAccuracy: 73.59%\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}